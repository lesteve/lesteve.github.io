<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Embarrassingly parallel for loops &mdash; joblib 0.9.0.dev0 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.9.0.dev0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="joblib 0.9.0.dev0 documentation" href="index.html" />
    <link rel="next" title="Development" href="developing.html" />
    <link rel="prev" title="On demand recomputing: the Memory class" href="memory.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="developing.html" title="Development"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="memory.html" title="On demand recomputing: the Memory class"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">joblib 0.9.0.dev0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="embarrassingly-parallel-for-loops">
<h1>Embarrassingly parallel for loops<a class="headerlink" href="#embarrassingly-parallel-for-loops" title="Permalink to this headline">¶</a></h1>
<div class="section" id="common-usage">
<h2>Common usage<a class="headerlink" href="#common-usage" title="Permalink to this headline">¶</a></h2>
<p>Joblib provides a simple helper class to write parallel for loops using
multiprocessing. The core idea is to write the code to be executed as a
generator expression, and convert it to parallel computing:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">sqrt</span><span class="p">(</span><span class="n">i</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="go">[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</span>
</pre></div>
</div>
<p>can be spread over 2 CPUs using the following:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">sqrt</span><span class="p">)(</span><span class="n">i</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="go">[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</span>
</pre></div>
</div>
<p>Under the hood, the <tt class="xref py py-class docutils literal"><span class="pre">Parallel</span></tt> object create a multiprocessing
<cite>pool</cite> that forks the Python interpreter in multiple processes to execute
each of the items of the list. The <cite>delayed</cite> function is a simple trick
to be able to create a tuple <cite>(function, args, kwargs)</cite> with a
function-call syntax.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>Under Windows, it is important to protect the main loop of code to
avoid recursive spawning of subprocesses when using joblib.Parallel.
In other words, you should be writing code like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="o">....</span>

<span class="k">def</span> <span class="nf">function1</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">def</span> <span class="nf">function2</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="o">...</span>

<span class="o">...</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c"># do stuff with imports and functions defined about</span>
    <span class="o">...</span>
</pre></div>
</div>
<p class="last"><strong>No</strong> code should <em>run</em> outside of the &#8220;if __name__ == &#8216;__main__&#8217;&#8221;
blocks, only imports and definitions.</p>
</div>
</div>
<div class="section" id="using-the-threading-backend">
<h2>Using the threading backend<a class="headerlink" href="#using-the-threading-backend" title="Permalink to this headline">¶</a></h2>
<p>By default <tt class="xref py py-class docutils literal"><span class="pre">Parallel</span></tt> uses the Python <tt class="docutils literal"><span class="pre">multiprocessing</span></tt> module to fork
separate Python worker processes to execute tasks concurrently on separate
CPUs. This is a reasonable default for generic Python programs but it induces
some overhead as the input and output data need to be serialized in a queue for
communication with the worker processes.</p>
<p>If you know that the function you are calling is based on a compiled extension
that releases the Python Global Interpreter Lock (GIL) during most of its
computation then it might be more efficient to use threads instead of Python
processes as concurrent workers. For instance this is the case if you write the
CPU intensive part of your code inside a <a href="#id1"><span class="problematic" id="id2">`with nogil`_</span></a> block of a Cython
function.</p>
<p>To use the threads, just pass <tt class="docutils literal"><span class="pre">&quot;threading&quot;</span></tt> as the value of the <tt class="docutils literal"><span class="pre">backend</span></tt>
parameter of the <tt class="xref py py-class docutils literal"><span class="pre">Parallel</span></tt> constructor:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s">&quot;threading&quot;</span><span class="p">)(</span>
<span class="gp">... </span>    <span class="n">delayed</span><span class="p">(</span><span class="n">sqrt</span><span class="p">)(</span><span class="n">i</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="go">[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</span>
</pre></div>
</div>
</div>
<div class="section" id="working-with-numerical-data-in-shared-memory-memmaping">
<h2>Working with numerical data in shared memory (memmaping)<a class="headerlink" href="#working-with-numerical-data-in-shared-memory-memmaping" title="Permalink to this headline">¶</a></h2>
<p>By default the workers of the pool are real Python processes forked using the
<tt class="docutils literal"><span class="pre">multiprocessing</span></tt> module of the Python standard library when <tt class="docutils literal"><span class="pre">n_jobs</span> <span class="pre">!=</span> <span class="pre">1</span></tt>.
The arguments passed as input to the <tt class="docutils literal"><span class="pre">Parallel</span></tt> call are serialized and
reallocated in the memory of each worker process.</p>
<p>This can be problematic for large arguments as they will be reallocated
<tt class="docutils literal"><span class="pre">n_jobs</span></tt> times by the workers.</p>
<p>As this problem can often occur in scientific computing with <tt class="docutils literal"><span class="pre">numpy</span></tt>
based datastructures, <a class="reference internal" href="#joblib.Parallel" title="joblib.Parallel"><tt class="xref py py-class docutils literal"><span class="pre">joblib.Parallel</span></tt></a> provides a special
handling for large arrays to automatically dump them on the filesystem
and pass a reference to the worker to open them as memory map
on that file using the <tt class="docutils literal"><span class="pre">numpy.memmap</span></tt> subclass of <tt class="docutils literal"><span class="pre">numpy.ndarray</span></tt>.
This makes it possible to share a segment of data between all the
worker processes.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The following only applies with the default <tt class="docutils literal"><span class="pre">&quot;multiprocessing&quot;</span></tt> backend. If
your code can release the GIL, then using <tt class="docutils literal"><span class="pre">backend=&quot;threading&quot;</span></tt> is even
more efficient.</p>
</div>
<div class="section" id="automated-array-to-memmap-conversion">
<h3>Automated array to memmap conversion<a class="headerlink" href="#automated-array-to-memmap-conversion" title="Permalink to this headline">¶</a></h3>
<p>The automated array to memmap conversion is triggered by a configurable
threshold on the size of the array:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib.pool</span> <span class="kn">import</span> <span class="n">has_shareable_memory</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_nbytes</span><span class="o">=</span><span class="mf">1e6</span><span class="p">)(</span>
<span class="gp">... </span>    <span class="n">delayed</span><span class="p">(</span><span class="n">has_shareable_memory</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">])</span>
<span class="go">[False, False, True]</span>
</pre></div>
</div>
<p>By default the data is dumped to the <tt class="docutils literal"><span class="pre">/dev/shm</span></tt> shared-memory partition if it
exists and writeable (typically the case under Linux). Otherwise the operating
system&#8217;s temporary folder is used. The location of the temporary data files can
be customized by passing a <tt class="docutils literal"><span class="pre">temp_folder</span></tt> argument to the <tt class="docutils literal"><span class="pre">Parallel</span></tt>
constructor.</p>
<p>Passing <tt class="docutils literal"><span class="pre">max_nbytes=None</span></tt> makes it possible to disable the automated array to
memmap conversion.</p>
</div>
<div class="section" id="manual-management-of-memmaped-input-data">
<h3>Manual management of memmaped input data<a class="headerlink" href="#manual-management-of-memmaped-input-data" title="Permalink to this headline">¶</a></h3>
<p>For even finer tuning of the memory usage it is also possible to
dump the array as an memmap directly from the parent process to
free the memory before forking the worker processes. For instance
let&#8217;s allocate a large array in the memory of the parent process:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">large_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
</pre></div>
</div>
<p>Dump it to a local file for memmaping:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">load</span><span class="p">,</span> <span class="n">dump</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">temp_folder</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_folder</span><span class="p">,</span> <span class="s">&#39;joblib_test.mmap&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span> <span class="n">os</span><span class="o">.</span><span class="n">unlink</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">dump</span><span class="p">(</span><span class="n">large_array</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">large_memmap</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">mmap_mode</span><span class="o">=</span><span class="s">&#39;r+&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">large_memmap</span></tt> variable is pointing to a <tt class="docutils literal"><span class="pre">numpy.memmap</span></tt>
instance:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">large_memmap</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">large_array</span><span class="o">.</span><span class="n">nbytes</span><span class="p">,</span> <span class="n">large_array</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(&#39;memmap&#39;, 8000000, (1000000,))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">large_array</span><span class="p">,</span> <span class="n">large_memmap</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>We can free the original array from the main process memory:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">del</span> <span class="n">large_array</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">gc</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>
</div>
<p>It it possible to slice <tt class="docutils literal"><span class="pre">large_memmap</span></tt> into a smaller memmap:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">small_memmap</span> <span class="o">=</span> <span class="n">large_memmap</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">small_memmap</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">small_memmap</span><span class="o">.</span><span class="n">nbytes</span><span class="p">,</span> <span class="n">small_memmap</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(&#39;memmap&#39;, 24, (3,))</span>
</pre></div>
</div>
<p>Finally we can also take a <tt class="docutils literal"><span class="pre">np.ndarray</span></tt> view backed on that same
memory mapped file:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">small_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">small_memmap</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">small_array</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">small_array</span><span class="o">.</span><span class="n">nbytes</span><span class="p">,</span> <span class="n">small_array</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(&#39;ndarray&#39;, 24, (3,))</span>
</pre></div>
</div>
<p>All those three datastructures point to the same memory buffer and
this same buffer will also be reused directly by the worker processes
of a <tt class="docutils literal"><span class="pre">Parallel</span></tt> call:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_nbytes</span><span class="o">=</span><span class="bp">None</span><span class="p">)(</span>
<span class="gp">... </span>    <span class="n">delayed</span><span class="p">(</span><span class="n">has_shareable_memory</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="n">large_memmap</span><span class="p">,</span> <span class="n">small_memmap</span><span class="p">,</span> <span class="n">small_array</span><span class="p">])</span>
<span class="go">[True, True, True]</span>
</pre></div>
</div>
<p>Note that here we used <tt class="docutils literal"><span class="pre">max_nbytes=None</span></tt> to disable the auto-dumping
feature of <tt class="docutils literal"><span class="pre">Parallel</span></tt>. The fact that <tt class="docutils literal"><span class="pre">small_array</span></tt> is still in
shared memory in the worker processes is a consequence of the fact
that it was already backed by shared memory in the parent process.
The pickling machinery of <tt class="docutils literal"><span class="pre">Parallel</span></tt> multiprocessing queues are
able to detect this situation and optimize it on the fly to limit
the number of memory copies.</p>
</div>
<div class="section" id="writing-parallel-computation-results-in-shared-memory">
<h3>Writing parallel computation results in shared memory<a class="headerlink" href="#writing-parallel-computation-results-in-shared-memory" title="Permalink to this headline">¶</a></h3>
<p>If you open your data using the <tt class="docutils literal"><span class="pre">w+</span></tt> or <tt class="docutils literal"><span class="pre">r+</span></tt> mode in the main program, the
worker will have <tt class="docutils literal"><span class="pre">r+</span></tt> mode access hence will be able to write results
directly to it alleviating the need to serialization to communicate back the
results to the parent process.</p>
<p>Here is an example script on parallel processing with preallocated
<tt class="docutils literal"><span class="pre">numpy.memmap</span></tt> datastructures:</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81</pre></div></td><td class="code"><div class="highlight"><pre><span class="sd">&quot;&quot;&quot;Demonstrate the usage of numpy.memmap with joblib.Parallel</span>

<span class="sd">This example shows how to preallocate data in memmap arrays both for input and</span>
<span class="sd">output of the parallel worker processes.</span>

<span class="sd">Sample output for this program::</span>

<span class="sd">    [Worker 93486] Sum for row 0 is -1599.756454</span>
<span class="sd">    [Worker 93487] Sum for row 1 is -243.253165</span>
<span class="sd">    [Worker 93488] Sum for row 3 is 610.201883</span>
<span class="sd">    [Worker 93489] Sum for row 2 is 187.982005</span>
<span class="sd">    [Worker 93489] Sum for row 7 is 326.381617</span>
<span class="sd">    [Worker 93486] Sum for row 4 is 137.324438</span>
<span class="sd">    [Worker 93489] Sum for row 8 is -198.225809</span>
<span class="sd">    [Worker 93487] Sum for row 5 is -1062.852066</span>
<span class="sd">    [Worker 93488] Sum for row 6 is 1666.334107</span>
<span class="sd">    [Worker 93486] Sum for row 9 is -463.711714</span>
<span class="sd">    Expected sums computed in the parent process:</span>
<span class="sd">    [-1599.75645426  -243.25316471   187.98200458   610.20188337   137.32443803</span>
<span class="sd">     -1062.85206633  1666.33410715   326.38161713  -198.22580876  -463.71171369]</span>
<span class="sd">    Actual sums computed by the worker processes:</span>
<span class="sd">    [-1599.75645426  -243.25316471   187.98200458   610.20188337   137.32443803</span>
<span class="sd">     -1062.85206633  1666.33410715   326.38161713  -198.22580876  -463.71171369]</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">load</span><span class="p">,</span> <span class="n">dump</span>


<span class="k">def</span> <span class="nf">sum_row</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the sum of a row in input and store it in output&quot;&quot;&quot;</span>
    <span class="n">sum_</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;[Worker </span><span class="si">%d</span><span class="s">] Sum for row </span><span class="si">%d</span><span class="s"> is </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">sum_</span><span class="p">))</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">folder</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
    <span class="n">samples_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="s">&#39;samples&#39;</span><span class="p">)</span>
    <span class="n">sums_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="s">&#39;sums&#39;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c"># Generate some data and an allocate an output buffer</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">)))</span>

        <span class="c"># Pre-allocate a writeable shared memory map as a container for the</span>
        <span class="c"># results of the parallel computation</span>
        <span class="n">sums</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">sums_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                         <span class="n">shape</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">&#39;w+&#39;</span><span class="p">)</span>

        <span class="c"># Dump the input data to disk to free the memory</span>
        <span class="n">dump</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">samples_name</span><span class="p">)</span>

        <span class="c"># Release the reference on the original in memory array and replace it</span>
        <span class="c"># by a reference to the memmap array so that the garbage collector can</span>
        <span class="c"># release the memory before forking. gc.collect() is internally called</span>
        <span class="c"># in Parallel just before forking.</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">samples_name</span><span class="p">,</span> <span class="n">mmap_mode</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>

        <span class="c"># Fork the worker processes to perform computation concurrently</span>
        <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">sum_row</span><span class="p">)(</span><span class="n">samples</span><span class="p">,</span> <span class="n">sums</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
                           <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

        <span class="c"># Compare the results from the output buffer with the ground truth</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Expected sums computed in the parent process:&quot;</span><span class="p">)</span>
        <span class="n">expected_result</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">expected_result</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Actual sums computed by the worker processes:&quot;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">sums</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected_result</span><span class="p">,</span> <span class="n">sums</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&quot;Failed to delete: &quot;</span> <span class="o">+</span> <span class="n">folder</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>Having concurrent workers write on overlapping shared memory data segments,
for instance by using inplace operators and assignments on a <cite>numpy.memmap</cite>
instance, can lead to data corruption as numpy does not offer atomic
operations. The previous example does not risk that issue as each task is
updating an exclusive segment of the shared result array.</p>
<p class="last">Some C/C++ compilers offer lock-free atomic primitives such as add-and-fetch
or compare-and-swap that could be exposed to Python via <a class="reference external" href="https://cffi.readthedocs.org">CFFI</a> for instance.
However providing numpy-aware atomic constructs is outside of the scope
of the joblib project.</p>
</div>
<p>A final note: don&#8217;t forget to clean up any temporary folder when you are done
with the computation:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">shutil</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">temp_folder</span><span class="p">)</span>
<span class="gp">... </span><span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">pass</span>  <span class="c"># this can sometimes fail under Windows</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="parallel-reference-documentation">
<h2><cite>Parallel</cite> reference documentation<a class="headerlink" href="#parallel-reference-documentation" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="joblib.Parallel">
<em class="property">class </em><tt class="descclassname">joblib.</tt><tt class="descname">Parallel</tt><big>(</big><em>n_jobs=1</em>, <em>backend=None</em>, <em>verbose=0</em>, <em>pre_dispatch='all'</em>, <em>temp_folder=None</em>, <em>max_nbytes=100000000.0</em>, <em>mmap_mode='r'</em><big>)</big><a class="headerlink" href="#joblib.Parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper class for readable parallel mapping.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_jobs: int</strong> :</p>
<blockquote>
<div><p>The number of jobs to use for the computation. If -1 all CPUs
are used. If 1 is given, no parallel computing code is used
at all, which is useful for debugging. For n_jobs below -1,
(n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all
CPUs but one are used.</p>
</div></blockquote>
<p><strong>backend: str or None</strong> :</p>
<blockquote>
<div><p>Specify the parallelization backend implementation.
Supported backends are:</p>
<blockquote>
<div><ul class="simple">
<li>&#8220;multiprocessing&#8221; used by default, can induce some
communication and memory overhead when exchanging input and
output data with the with the worker Python processes.</li>
<li>&#8220;threading&#8221; is a very low-overhead backend but it suffers
from the Python Global Interpreter Lock if the called function
relies a lot on Python objects. &#8220;threading&#8221; is mostly useful
when the execution bottleneck is a compiled extension that
explicitly releases the GIL (for instance a Cython loop wrapped
in a &#8220;with nogil&#8221; block or an expensive call to a library such
as NumPy).</li>
</ul>
</div></blockquote>
</div></blockquote>
<p><strong>verbose: int, optional</strong> :</p>
<blockquote>
<div><p>The verbosity level: if non zero, progress messages are
printed. Above 50, the output is sent to stdout.
The frequency of the messages increases with the verbosity level.
If it more than 10, all iterations are reported.</p>
</div></blockquote>
<p><strong>pre_dispatch: {&#8216;all&#8217;, integer, or expression, as in &#8216;3*n_jobs&#8217;}</strong> :</p>
<blockquote>
<div><p>The amount of jobs to be pre-dispatched. Default is &#8216;all&#8217;,
but it may be memory consuming, for instance if each job
involves a lot of a data.</p>
</div></blockquote>
<p><strong>temp_folder: str, optional</strong> :</p>
<blockquote>
<div><p>Folder to be used by the pool for memmaping large arrays
for sharing memory with worker processes. If None, this will try in
order:
- a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,
- /dev/shm if the folder exists and is writable: this is a RAMdisk</p>
<blockquote>
<div><p>filesystem available by default on modern Linux distributions,</p>
</div></blockquote>
<ul class="simple">
<li>the default system temporary folder that can be overridden
with TMP, TMPDIR or TEMP environment variables, typically /tmp
under Unix operating systems.</li>
</ul>
<p>Only active when backend=&#8221;multiprocessing&#8221;.</p>
</div></blockquote>
<p><strong>max_nbytes int, str, or None, optional, 100e6 (100MB) by default</strong> :</p>
<blockquote class="last">
<div><p>Threshold on the size of arrays passed to the workers that
triggers automated memory mapping in temp_folder. Can be an int
in Bytes, or a human-readable string, e.g., &#8216;1M&#8217; for 1 megabyte.
Use None to disable memmaping of large arrays.
Only active when backend=&#8221;multiprocessing&#8221;.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>This object uses the multiprocessing module to compute in
parallel the application of a function to many different
arguments. The main functionality it brings in addition to
using the raw multiprocessing API are (see examples for details):</p>
<blockquote>
<div><ul>
<li><p class="first">More readable code, in particular since it avoids
constructing list of arguments.</p>
</li>
<li><dl class="first docutils">
<dt>Easier debugging:</dt>
<dd><ul class="first last simple">
<li>informative tracebacks even when the error happens on
the client side</li>
<li>using &#8216;n_jobs=1&#8217; enables to turn off parallel computing
for debugging without changing the codepath</li>
<li>early capture of pickling errors</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">An optional progress meter.</p>
</li>
<li><p class="first">Interruption of multiprocesses jobs with &#8216;Ctrl-C&#8217;</p>
</li>
<li><p class="first">Flexible pickling control for the communication to and from
the worker processes.</p>
</li>
<li><p class="first">Ability to use shared memory efficiently with worker
processes for large numpy-based datastructures.</p>
</li>
</ul>
</div></blockquote>
<p class="rubric">Examples</p>
<p>A simple example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">sqrt</span><span class="p">)(</span><span class="n">i</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="go">[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]</span>
</pre></div>
</div>
<p>Reshaping the output when the function has several return
values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">modf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">modf</span><span class="p">)(</span><span class="n">i</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">r</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span>
<span class="go">(0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span>
<span class="go">(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)</span>
</pre></div>
</div>
<p>The progress meter: the higher the value of <cite>verbose</cite>, the more
messages:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">5</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">sleep</span><span class="p">)(</span><span class="o">.</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span> 
<span class="go">[Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s</span>
<span class="go">[Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s</span>
<span class="go">[Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s</span>
<span class="go">[Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s</span>
<span class="go">[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished</span>
</pre></div>
</div>
<p>Traceback example, note how the line of the error is indicated
as well as the values of the parameter passed to the function that
triggered the exception, even though the traceback happens in the
child process:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">heapq</span> <span class="kn">import</span> <span class="n">nlargest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">nlargest</span><span class="p">)(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="s">&#39;abcde&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> 
<span class="go">#...</span>
<span class="go">---------------------------------------------------------------------------</span>
<span class="go">Sub-process traceback:</span>
<span class="go">---------------------------------------------------------------------------</span>
<span class="go">TypeError                                          Mon Nov 12 11:37:46 2012</span>
<span class="go">PID: 12934                                    Python 2.7.3: /usr/bin/python</span>
<span class="go">...........................................................................</span>
<span class="go">/usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)</span>
<span class="go">    419         if n &gt;= size:</span>
<span class="go">    420             return sorted(iterable, key=key, reverse=True)[:n]</span>
<span class="go">    421</span>
<span class="go">    422     # When key is none, use simpler decoration</span>
<span class="go">    423     if key is None:</span>
<span class="go">--&gt; 424         it = izip(iterable, count(0,-1))                    # decorate</span>
<span class="go">    425         result = _nlargest(n, it)</span>
<span class="go">    426         return map(itemgetter(0), result)                   # undecorate</span>
<span class="go">    427</span>
<span class="go">    428     # General case, slowest method</span>

<span class="go">TypeError: izip argument #1 must support iteration</span>
<span class="go">___________________________________________________________________________</span>
</pre></div>
</div>
<p>Using pre_dispatch in a producer/consumer situation, where the
data is generated on the fly. Note how the producer is first
called a 3 times before the parallel loop is initiated, and then
called to generate new data on the fly. In this case the total
number of iterations cannot be reported in the progress messages:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">producer</span><span class="p">():</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Produced </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">yield</span> <span class="n">i</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="s">&#39;1.5*n_jobs&#39;</span><span class="p">)(</span>
<span class="gp">... </span>                        <span class="n">delayed</span><span class="p">(</span><span class="n">sqrt</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">producer</span><span class="p">())</span> 
<span class="go">Produced 0</span>
<span class="go">Produced 1</span>
<span class="go">Produced 2</span>
<span class="go">[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    0.0s</span>
<span class="go">Produced 3</span>
<span class="go">[Parallel(n_jobs=2)]: Done   2 jobs       | elapsed:    0.0s</span>
<span class="go">Produced 4</span>
<span class="go">[Parallel(n_jobs=2)]: Done   3 jobs       | elapsed:    0.0s</span>
<span class="go">Produced 5</span>
<span class="go">[Parallel(n_jobs=2)]: Done   4 jobs       | elapsed:    0.0s</span>
<span class="go">[Parallel(n_jobs=2)]: Done   5 out of   6 | elapsed:    0.0s remaining:    0.0s</span>
<span class="go">[Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed:    0.0s finished</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Embarrassingly parallel for loops</a><ul>
<li><a class="reference internal" href="#common-usage">Common usage</a></li>
<li><a class="reference internal" href="#using-the-threading-backend">Using the threading backend</a></li>
<li><a class="reference internal" href="#working-with-numerical-data-in-shared-memory-memmaping">Working with numerical data in shared memory (memmaping)</a><ul>
<li><a class="reference internal" href="#automated-array-to-memmap-conversion">Automated array to memmap conversion</a></li>
<li><a class="reference internal" href="#manual-management-of-memmaped-input-data">Manual management of memmaped input data</a></li>
<li><a class="reference internal" href="#writing-parallel-computation-results-in-shared-memory">Writing parallel computation results in shared memory</a></li>
</ul>
</li>
<li><a class="reference internal" href="#parallel-reference-documentation"><cite>Parallel</cite> reference documentation</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="memory.html"
                        title="previous chapter">On demand recomputing: the <cite>Memory</cite> class</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="developing.html"
                        title="next chapter">Development</a></p>

    <hr/>

<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
    <hr/>
    <div>
	<h3>Mailing list</h3>
	<a href="http://librelist.com/browser/joblib/">joblib@librelist.com</a>
    <p class="searchtip" style="font-size: 80%">
    Send an email to subscribe</p>
    </div>
<hr/>
<small><a href="_sources/parallel.txt"
           rel="nofollow">Show this page source</a>
</small>

        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="developing.html" title="Development"
             >next</a> |</li>
        <li class="right" >
          <a href="memory.html" title="On demand recomputing: the Memory class"
             >previous</a> |</li>
        <li><a href="index.html">joblib 0.9.0.dev0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2008-2009, Gael Varoquaux.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>